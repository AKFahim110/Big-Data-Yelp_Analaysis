{
  "metadata": {
    "name": "Comprehensive Analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, avg, desc, rank\nfrom pyspark.sql.window import Window\n\n# Create a SparkSession\nspark \u003d SparkSession.builder \\\n    .appName(\"Top 5 Merchants in Each City\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Load the necessary datasets from Hive\ndf_review \u003d spark.table(\u0027review\u0027)\ndf_checkin \u003d spark.table(\u0027checkin\u0027)\ndf_business \u003d spark.table(\u0027business\u0027)  # Add this line to read the business table\n\n# Calculate rating frequency by merchant\nrating_frequency \u003d df_review.groupBy(\"rev_business_id\").agg(count(\"*\").alias(\"rating_frequency\"))\n\n# Calculate average rating by merchant\navg_rating \u003d df_review.groupBy(\"rev_business_id\").agg(avg(\"rev_stars\").alias(\"avg_rating\"))\n\n# Calculate check-in frequency by merchant\ncheckin_frequency \u003d df_checkin.groupBy(\"business_id\").agg(count(\"*\").alias(\"checkin_frequency\"))\n\n# Join with the business table to add the year\ncombined_data \u003d rating_frequency.join(avg_rating, \"rev_business_id\") \\\n    .join(checkin_frequency, rating_frequency[\"rev_business_id\"] \u003d\u003d checkin_frequency[\"business_id\"], \"inner\") \\\n    .join(df_business.select(\"business_id\", \"name\", \"city\"), rating_frequency[\"rev_business_id\"] \u003d\u003d df_business[\"business_id\"], \"inner\") \\\n    .drop(checkin_frequency[\"business_id\"]) \\\n    .drop(df_business[\"business_id\"])  # Drop duplicate columns\n\n# Define window specification for ranking within each city\nwindow_spec \u003d Window.partitionBy(\"city\").orderBy(desc(\"rating_frequency\"), desc(\"avg_rating\"), desc(\"checkin_frequency\"))\n\n# Rank merchants within each city based on rating frequency, average rating, and check-in frequency\nranked_merchants \u003d combined_data.withColumn(\"rank_within_city\", rank().over(window_spec))\n\n# Filter to get only top 5 merchants in each city\ntop_merchants_in_each_city \u003d ranked_merchants.filter(col(\"rank_within_city\") \u003c\u003d 5)\n\n# Show the top 5 merchants in each city based on rating frequency, average rating, and check-in frequency\nz.show(top_merchants_in_each_city)\n\n# Stop the SparkSession\nspark.stop()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}