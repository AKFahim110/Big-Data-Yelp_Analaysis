{
  "metadata": {
    "name": "Rating Analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n# 1. Analyze the distribution of ratings (1-5)\r\n\r\n# Import necessary modules\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import count, col\r\n\r\n# Create SparkSession\r\nspark \u003d SparkSession.builder \\\r\n    .appName(\"Rating Distribution Analysis\") \\\r\n    .getOrCreate()\r\n\r\n# Read the \u0027review\u0027 table from Hive\r\nreview_df \u003d spark.sql(\"SELECT * FROM review\")\r\n\r\n# Calculate the distribution of ratings\r\nrating_distribution \u003d review_df.groupBy(\u0027rev_stars\u0027) \\\r\n    .agg(count(\u0027*\u0027).alias(\u0027count\u0027)) \\\r\n    .orderBy(\u0027rev_stars\u0027)\r\n\r\n# Show the distribution of ratings\r\nz.show(rating_distribution)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n# 2. Show the frequency of ratings on each day of the week\r\n\r\n# Import necessary functions\r\nfrom pyspark.sql.functions import date_format, count\r\n\r\n# Calculate the frequency of ratings on each day of the week\r\nratings_by_day \u003d review_df.groupBy(date_format(\u0027rev_date\u0027, \u0027E\u0027).alias(\u0027day_of_week\u0027)) \\\r\n                           .agg(count(\u0027rev_stars\u0027).alias(\u0027rating_count\u0027)) \\\r\n                           .orderBy(\u0027day_of_week\u0027)\r\n\r\n\r\nz.show(ratings_by_day)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 3. Identify the top 5 merchants with the most 5-star ratings.\n\n# Import necessary modules\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count, col\n\n# Create SparkSession\nspark \u003d SparkSession.builder \\\n    .appName(\"Top 5 Merchants with Most 5-Star Ratings\") \\\n    .getOrCreate()\n\n# Read the \u0027review\u0027 table from Hive\nreview_df \u003d spark.sql(\"SELECT * FROM review\")\n\n# Identify the top 5 merchants with the most 5-star ratings\ntop_merchants \u003d review_df.filter(review_df[\u0027rev_stars\u0027] \u003d\u003d 5) \\\n                         .groupBy(\u0027rev_business_id\u0027) \\\n                         .agg(count(\u0027*\u0027).alias(\u0027five_star_count\u0027)) \\\n                         .orderBy(col(\u0027five_star_count\u0027).desc()) \\\n                         .limit(5)\n\n# Show the top 5 merchants with the most 5-star ratings\nz.show(top_merchants)\n\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}