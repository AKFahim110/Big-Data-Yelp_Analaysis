{
  "metadata": {
    "name": "Review Analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql import HiveContext\r\n\r\n# Create a HiveContext\r\nhc \u003d HiveContext(sc)\r\n\r\n# Set the database name\r\n\r\n# Execute SQL query to count the yearly number of reviews\r\nresult \u003d hc.sql(f\"\"\"\r\n    SELECT YEAR(rev_date) AS year, COUNT(*) AS review_count\r\n    FROM review\r\n    GROUP BY YEAR(rev_date)\r\n    ORDER BY year\r\n\"\"\")\r\n\r\n# Show the result\r\nz.show(result)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import HiveContext\n\n# Create a HiveContext\nhc \u003d HiveContext(sc)\n\n\n# Execute SQL query to summarize the count of helpful, funny, and cool reviews each year\nresult \u003d hc.sql(f\"\"\"\n    SELECT \n        YEAR(rev_date) AS year, \n        COUNT(*) AS total_reviews,\n        SUM(rev_useful) AS total_helpful_reviews,\n        SUM(rev_funny) AS total_funny_reviews,\n        SUM(rev_cool) AS total_cool_reviews\n    FROM review\n    GROUP BY YEAR(rev_date)\n    ORDER BY year\n\"\"\")\n\n# Show the result\nz.show(result)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import HiveContext\n\n# Create a HiveContext\nhc \u003d HiveContext(sc)\n\n\n# Execute SQL query to create a ranking of users based on their total reviews each year\nresult \u003d hc.sql(f\"\"\"\n    SELECT \n        year,\n        rev_user_id,\n        total_reviews,\n        RANK() OVER (PARTITION BY year ORDER BY total_reviews DESC) AS user_rank\n    FROM (\n        SELECT \n            YEAR(rev_date) AS year,\n            rev_user_id,\n            COUNT(*) AS total_reviews\n        FROM review\n        GROUP BY YEAR(rev_date), rev_user_id\n    ) ranked_reviews\n    ORDER BY year, user_rank\n\"\"\")\n\n# Show the result\nz.show(result)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import HiveContext\n\n# Create a HiveContext\nhc \u003d HiveContext(sc)\n\n# Execute SQL query to extract the top 20 common words from reviews\nresult \u003d hc.sql(f\"\"\"\n    SELECT word, COUNT(*) AS word_count\n    FROM (\n        SELECT EXPLODE(SPLIT(rev_text, \u0027 \u0027)) AS word\n        FROM review\n    ) words\n    GROUP BY word\n    ORDER BY word_count DESC\n    LIMIT 20\n\"\"\")\n\n# Show the result\nz.show(result)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql import HiveContext\r\nfrom pyspark.sql.functions import explode\r\nfrom pyspark.ml.feature import RegexTokenizer\r\nfrom pyspark.ml import Pipeline\r\nimport nltk\r\nfrom nltk.corpus import stopwords\r\nfrom nltk import pos_tag\r\nfrom wordcloud import WordCloud\r\nimport matplotlib.pyplot as plt\r\n\r\n# Initialize NLTK\r\nnltk.download(\u0027stopwords\u0027)\r\nnltk.download(\u0027averaged_perceptron_tagger\u0027)\r\n\r\n# Create a HiveContext\r\nhc \u003d HiveContext(sc)\r\n\r\n# Set the database name\r\ndatabase_name \u003d \"your_database_name\"\r\n\r\n# Load the reviews data from Hive\r\nreviews_df \u003d hc.table(f\"{database_name}.review\")\r\n\r\n# Tokenize the reviews into words\r\ntokenizer \u003d RegexTokenizer(inputCol\u003d\"rev_text\", outputCol\u003d\"words\", pattern\u003d\"\\\\W\")\r\ntokenized_df \u003d tokenizer.transform(reviews_df)\r\n\r\n# Define the function to filter words based on POS tags\r\ndef filter_words(words):\r\n    filtered_words \u003d []\r\n    for word, pos in pos_tag(words):\r\n        # Filter out non-alphabetic words and stopwords\r\n        if word.isalpha() and word not in stopwords.words(\u0027english\u0027):\r\n            # Filter out only nouns, adjectives, and verbs\r\n            if pos.startswith(\u0027NN\u0027) or pos.startswith(\u0027JJ\u0027) or pos.startswith(\u0027VB\u0027):\r\n                filtered_words.append(word.lower())\r\n    return filtered_words\r\n\r\n# Apply the filtering function to the tokenized words\r\nfilter_udf \u003d udf(filter_words, ArrayType(StringType()))\r\nfiltered_words_df \u003d tokenized_df.withColumn(\"filtered_words\", filter_udf(\"words\"))\r\n\r\n# Explode the filtered words to create one row per word\r\nexploded_df \u003d filtered_words_df.select(explode(\"filtered_words\").alias(\"word\"))\r\n\r\n# Count the occurrences of each filtered word\r\nword_counts \u003d exploded_df.groupBy(\"word\").count().orderBy(\"count\", ascending\u003dFalse)\r\n\r\n# Convert the result to Pandas for generating word cloud\r\nword_counts_pandas \u003d word_counts.toPandas()\r\n\r\n# Generate the word cloud\r\nwordcloud \u003d WordCloud(width\u003d800, height\u003d40\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import HiveContext\n\n# Create a HiveContext\nhc \u003d HiveContext(sc)\n\n\n# Define the selected words\nselected_words \u003d [\"Chinese\", \"steak\"]\n\n# Execute SQL query to find co-occurrences of selected words\nresult \u003d hc.sql(f\"\"\"\n    SELECT word1, word2, COUNT(*) AS co_occurrences\n    FROM (\n        SELECT EXPLODE(SPLIT(word, \u0027 \u0027)) AS word1, word2\n        FROM (\n            SELECT CONCAT_WS(\u0027 \u0027, SPLIT(rev_text, \u0027 \u0027)) AS word\n            FROM review\n            WHERE {\u0027 OR \u0027.join([f\"rev_text LIKE \u0027%{word}%\u0027\" for word in selected_words])}\n        ) t1\n    ) t2\n    WHERE word1 !\u003d word2\n    GROUP BY word1, word2\n    ORDER BY co_occurrences DESC\n\"\"\")\n\n# Show the result\nz.show(result)\n\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}