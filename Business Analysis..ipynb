{
  "metadata": {
    "name": "Business Analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#1 Identify the most common merchants in the United States (Top 20).\n\nfrom pyspark import HiveContext\nfrom pyspark.sql.functions import count, col\n\nhc \u003d HiveContext(sc)\ndf \u003d hc.table(\u0027business\u0027)\n\nresult \u003d df.groupBy(\u0027name\u0027)\\\n    .agg(count(\u0027name\u0027).alias(\u0027cnt\u0027))\\\n    .orderBy(col(\u0027cnt\u0027).desc())\\\n    .limit(20)\n\nz.show(result)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#2 Find the top 10 cities with the most merchants in the United States.\n\n\nfrom pyspark.sql import HiveContext\nfrom pyspark.sql.functions import count, col\nsc \u003d spark.sparkContext\nhc \u003d HiveContext(sc)\ndf \u003d hc.table(\u0027business\u0027)\n\n# Perform aggregation by city\nresult \u003d df.groupBy(\u0027city\u0027) \\\n            .agg(count(\u0027name\u0027).alias(\u0027merchant_count\u0027)) \\\n            .orderBy(col(\u0027merchant_count\u0027).desc()) \\\n            .limit(10)\n\nz.show(result)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#3 Identify the top 5 states with the most merchants in the United States.\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count, col\nsc \u003d spark.sparkContext\nhc \u003d HiveContext(sc)\ndf \u003d hc.table(\u0027business\u0027)\n# Perform aggregation by state\nresult \u003d df.groupBy(\u0027state\u0027) \\\n                      .agg(count(\u0027name\u0027).alias(\u0027merchant_count\u0027)) \\\n                      .orderBy(col(\u0027merchant_count\u0027).desc()) \\\n                      .limit(10)\n\nz.show(result)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#4 Find the most common merchants in the United States and display their average ratings (Top 20).\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count, col, avg\nsc \u003d spark.sparkContext\nhc \u003d HiveContext(sc)\ndf \u003d hc.table(\u0027business\u0027)\nresult \u003d df.groupBy(\u0027name\u0027) \\\n                      .agg(count(\u0027name\u0027).alias(\u0027merchant_count\u0027), avg(\u0027stars\u0027).alias(\u0027avg_rating\u0027)) \\\n                      .orderBy(col(\u0027merchant_count\u0027).desc()) \\\n                      .limit(20)\n\nz.show(result)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n#5 Analyze and list the top 10 cities with the highest average ratings.\r\n\r\n\r\nfrom pyspark import HiveContext\r\nfrom pyspark.sql.functions import count, col, countDistinct\r\n\r\nhc \u003d HiveContext(sc)\r\ndf \u003d hc.table(\u0027business\u0027)\r\nresult \u003d df.filter(df[\u0027is_open\u0027] \u003d\u003d 1)\\\r\n    .groupBy(\u0027city\u0027)\\\r\n    .agg((count(\u0027review_count\u0027) / countDistinct(\u0027name\u0027)).alias(\u0027avg_review_count_per_merchant\u0027))\\\r\n    .orderBy(col(\u0027avg_review_count_per_merchant\u0027).desc())\\\r\n    .limit(15)\r\n\r\nz.show(result)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n#6 Count the number of categories.\r\n\r\n\r\nfrom pyspark import HiveContext\r\nfrom pyspark.sql.functions import explode, split, count, desc\r\n\r\nhc \u003d HiveContext(sc)\r\ndf \u003d hc.table(\u0027business\u0027)\r\n\r\ncategory_counts \u003d df.select(explode(split(\u0027categories\u0027, \u0027,\u0027)).alias(\u0027category\u0027)) \\\r\n    .groupBy(\u0027category\u0027) \\\r\n    .agg(count(\u0027*\u0027).alias(\u0027count\u0027)) \\\r\n    .orderBy(desc(\u0027count\u0027)) \\\r\n    .limit(20)\r\n\r\nz.show(category_counts)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n#7\r\n\r\nfrom pyspark import HiveContext\r\nfrom pyspark.sql.functions import count, col, split, explode\r\n\r\nhc \u003d HiveContext(sc)\r\ndf \u003d hc.table(\u0027business\u0027)\r\n\r\nresult \u003d df.select(explode(split(\u0027categories\u0027, \u0027, \u0027)).alias(\u0027category\u0027))\\\r\n    .groupBy(\u0027category\u0027) \\\r\n    .agg(count(\u0027category\u0027).alias(\u0027cnt\u0027))\\\r\n    .orderBy(col(\u0027cnt\u0027).desc())\\\r\n    .limit(20)\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#8\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import sum\nsc \u003d spark.sparkContext\nhc \u003d HiveContext(sc)\ndf \u003d hc.table(\u0027business\u0027)\n\n\n# Filter businesses with five-star reviews\nfive_star_businesses \u003d df.filter(df.stars \u003d\u003d 5)\n\nresult \u003d five_star_businesses.groupBy(\u0027name\u0027) \\\n                              .agg(sum(\u0027review_count\u0027).alias(\u0027five_star_reviews\u0027)) \\\n                              .orderBy(col(\u0027five_star_reviews\u0027).desc()) \\\n                              .limit(20)\n\nz.show(result)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n#9\r\n\r\nfrom pyspark.sql import HiveContext\r\nfrom pyspark.sql.functions import col, when\r\n\r\n# Create HiveContext\r\nhc \u003d HiveContext(sc)\r\n\r\n# Load the \u0027business\u0027 table\r\nbusiness_df \u003d hc.table(\u0027business\u0027)\r\n\r\n# Define the cuisine types\r\ncuisines \u003d [\u0027Chinese\u0027, \u0027American\u0027, \u0027Mexican\u0027]\r\n\r\n# Create columns for each cuisine type indicating whether the business belongs to that cuisine\r\nbusiness_with_cuisines \u003d business_df.withColumn(\u0027Chinese\u0027, when(col(\u0027categories\u0027).contains(\u0027Chinese\u0027), 1).otherwise(0)) \\\r\n                                   .withColumn(\u0027American\u0027, when(col(\u0027categories\u0027).contains(\u0027American\u0027), 1).otherwise(0)) \\\r\n                                   .withColumn(\u0027Mexican\u0027, when(col(\u0027categories\u0027).contains(\u0027Mexican\u0027), 1).otherwise(0))\r\n\r\n# Summarize the types and quantities of restaurants for different cuisines\r\ncuisine_summary \u003d business_with_cuisines.selectExpr(\u0027sum(Chinese) as Chinese\u0027,\r\n                                                    \u0027sum(American) as American\u0027,\r\n                                                    \u0027sum(Mexican) as Mexican\u0027)\r\n\r\nz.show(cuisine_summary)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n#10 \r\n\r\nfrom pyspark import HiveContext\r\nfrom pyspark.sql.functions import col\r\n\r\nhc \u003d HiveContext(sc)\r\ndf \u003d hc.table(\u0027business\u0027)\r\n\r\n# Filter restaurants with specified cuisines\r\ncuisine_types \u003d [\u0027Chinese\u0027, \u0027American\u0027, \u0027Mexican\u0027]\r\nrestaurant_review_counts \u003d df.select(\u0027categories\u0027, \u0027review_count\u0027) \\\r\n    .filter(df[\u0027categories\u0027].like(\u0027%Chinese%\u0027) | df[\u0027categories\u0027].like(\u0027%American%\u0027) | df[\u0027categories\u0027].like(\u0027%Mexican%\u0027)) \\\r\n    .orderBy(col(\u0027review_count\u0027).desc()) \\\r\n    .limit(20)\r\n\r\nz.show(restaurant_review_counts)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n#11\r\n\r\nfrom pyspark import HiveContext\r\nfrom pyspark.sql.functions import col\r\n\r\nhc \u003d HiveContext(sc)\r\ndf \u003d hc.table(\u0027business\u0027)\r\n\r\n# Filter restaurants with specified cuisines\r\ncuisine_types \u003d [\u0027Chinese\u0027, \u0027American\u0027, \u0027Mexican\u0027]\r\nrestaurant_ratings \u003d df.select(\u0027categories\u0027, \u0027stars\u0027) \\\r\n    .filter(df[\u0027categories\u0027].like(\u0027%Chinese%\u0027) | df[\u0027categories\u0027].like(\u0027%American%\u0027) | df[\u0027categories\u0027].like(\u0027%Mexican%\u0027)) \\\r\n    .orderBy(col(\u0027stars\u0027).desc()) \\\r\n    .limit(20)\r\n\r\nz.show(restaurant_ratings)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#12  Determine the average latitude and longitude coordinates for businesses in each state:\nfrom pyspark import HiveContext\nfrom pyspark.sql.functions import count, col\nhc \u003d HiveContext(sc)\ndf \u003d hc.table(\u0027business\u0027)\nresult \u003d df.groupBy(\u0027state\u0027)\\\n    .agg(avg(\u0027latitude\u0027).alias(\u0027avg_latitude\u0027), avg(\u0027longitude\u0027).alias(\u0027avg_longitude\u0027))\n\nz.show(result)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}